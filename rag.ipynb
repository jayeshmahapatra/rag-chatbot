{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://jayeshmahapatra.github.io/2023/06/22/arcface.html\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayesh/miniconda3/envs/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"zephyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArcFace loss is commonly used in training face recognition systems due to its ability to achieve enhanced embedding separability compared to traditional softmax losses. By explicitly considering the angles between embeddings, ArcFace loss promotes angular separability of embeddings belonging to different classes while enforcing intra-class angular similarity. This results in more cleanly separated and compact embedding clusters during training, as demonstrated in visualizations provided by a GitHub repository called ArcFace-Embedding-Visualization. The use of ArcFace loss can be beneficial for tasks that require high degrees of inter-class separability, such as face recognition.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What use arcface loss?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What use arcface loss?',\n",
       " 'context': [Document(page_content=\"\\\\[L_3 = -log \\\\frac{e^{s \\\\cos (\\\\theta_{y_i} + m) }}{e^{s\\\\cos (\\\\theta_{y_i} + m)} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nThis is the ArcFace Loss that is widely used in training face recognition systems.\\n\\nExperimental Results\\nIn order to demonstrate the difference in class separation when using arcface loss vs classic softmax, I have created a github repository called ArcFace-Embedding-Visualization.\\nThis repository contains contains code for visualizing how embeddings evolve when trained using ArcFace vs Softmax Loss, as shown below:\\n\\nVisualization of Embedding Separation across Training Epochs\\n\\n\\n\\n\\n\\n            ArcFace 3D embeddings during training.\\n         \\n\\n\\n\\n            Softmax 3D embeddings during training.\\n         \\n\\n\\n\\nAs we can see that the arcface loss results in embedding clusters that are more cleanly separated as well as are more compact than the embeddings trained using classic softmax.\\nThe provided code trains a VGG8 network on the popular MNIST dataset using the ArcFace loss and Softmax loss, and generates visualization of embeddings. For more information on how to create and visualize such embeddings, please checkout the repository.\\nConclusion\\nIn conclusion, in this blog post we introduced ArcFace loss as a powerful technique for achieving enhanced embedding separability. By explicitly considering the angles between embeddings, ArcFace loss provides improved discrimination of embeddings, especially in face recognition tasks.\\nReferences\\n\\nJiankang Deng, , Jia Guo, and Stefanos Zafeiriou. “ArcFace: Additive Angular Margin Loss for Deep Face Recognition”.CoRR abs/1801.07698 (2018).\\n\\n\\n\\n<Previous PostBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\n>Next PostAndroid OCR Translation App using Kotlin and Google ML-Kit\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\nJayesh Mahapatrajayeshmahapatra@gmail.com\\n\\n jayeshmahapatra jayeshmahapatra jayeshmahapatra\\n\\n\\nJayesh's techincal blog where he writes about Machine Learning and Computer Science.\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en'}),\n",
       "  Document(page_content=\"Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnhancing Embedding Separation with ArcFace Loss\\n\\nJun 22, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEmbeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.\\nEmbeddings can be obtained from Neural Networks trained with traditional classification losses. However, these losses do not explicitly optimize cosine distances to achieve both inter-class separability and intra-class compactness.\\nIn this article, we will delve into ArcFace loss and its suitability for tasks that require high degree of inter-class separability while still having high intra-class variance, such as face recognition.\\n\\nEmbeddings & Separability\\nBefore we start discussing losses, let’s take a refresher on what embeddings are:\\n\\nEmbeddings are mathematical representations of objects in a vector space.\\nThey can usually be obtained by training a Neural Network.\\nSimilar objects have embeddings that are close to each other.\\nThe similarity is measured using distance metrics like euclidean or cosine distance.\\nWe usually want embeddings such that objects belonging to the same class are close together whereas objects belonging to different classes are further apart.\\n\\n\\n\\n\\nFigure: Visualization of embeddings of audio clips speaking a certain word.  Source: Study of Acoustic Word Embeddings in relation to Mishearing of Words, Jayesh Mahapatra 2021\\n\\n\\n\\n\\nEmbedding separation with Softmax Loss\\nClassical Softmax Loss looks like this\\n\\n\\\\[L_{1} = -log \\\\frac{e^{W^T_{y_i}x_i + b_{y_i}}}{\\\\sum^N_{j=1}e^{W^T_jx_i + b_j}}\\\\]\\n\\nLet’s break down the equation to understand the individual elements:\\n\\nFeatures: $x_i \\\\in \\\\mathbb{R}^d$ represents features of $i$-th example of the $y$-th class.\\nWeights: $W_j \\\\in \\\\mathbb{R}^d$ is the $j$-th column of weight matrix $W \\\\in \\\\mathbb{R}^{d \\\\times N}$.\\nBias: $b_j \\\\in \\\\mathbb{R}^N$ is the bias term.\\nNumber of Classes: $N$ denotes the number of unique classes in the dataset.\\n\\nSince we are not constraining the weight matrix $W$ or the features $x_i$ in anyway, the loss function does not explicitly consider the angular relationship between embeddings. This means that optimizing for this loss doesn’t explicitly promote angular separibility of embeddings belonging to different classes, neither does it explicitly enforce intra-class angular similarity.\\n\\nArcFace Loss\\nLet’s now modify the softmax loss to explicitly care about the angles between embeddings.\\nFor simplicity we can set the bias term $b_j = 0$. \\nThen we can rewrite $W^T_jx_i = \\\\Vert W_j \\\\Vert \\\\Vert x_i \\\\Vert cos \\\\theta_j $ , where $\\\\theta_j$ is the angle between the weight $W_j$ and feature $x_j$.\\nOur goal is to modify the loss function such that the predictions only depend on $\\\\theta_j$.\\nWe can achieve this by normalizing the other two terms in the equation:\\n\\nWe set $\\\\Vert W_j \\\\Vert = 1$ using $l_2$ normalization\\nWe normalize features $ \\\\Vert x_i \\\\Vert $ by $l_2$ normalization and re-scale it by $s$.\\n\\nFollowing these modifications the loss function becomes:\\n\\n\\\\[L_2 = -log \\\\frac{e^{s\\\\cos \\\\theta_{y_i}}}{e^{s\\\\cos \\\\theta_{y_i}} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nNow the predictions only depend on $\\\\theta$, and the features are distributed on a hypersphere with radius $s$. The scaling parameter $s$ is used to control the overall range of output logits which in term affects how large the gradients will be during training.\\nWe can improve this further by introducing an additive angular margin parameter $m$ to the angle $\\\\theta$ between $W_{y_i}$ and $x_i$. A margin encourages better class separation by penalizing the loss function if examples fall within a certain\\ndistance from the decision boundary. In our case, since we are using angular distances, our margin $m$ is also angular.\\nThe final loss then becomes:\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en'})],\n",
       " 'answer': 'ArcFace loss is commonly used in training face recognition systems due to its ability to achieve enhanced embedding separability compared to traditional softmax losses. By explicitly considering the angles between embeddings, ArcFace loss promotes angular separability of embeddings belonging to different classes while enforcing intra-class angular similarity. This results in more cleanly separated and compact embedding clusters during training, as demonstrated in visualizations provided by a GitHub repository called ArcFace-Embedding-Visualization. The use of ArcFace loss can be beneficial for tasks that require high degrees of inter-class separability, such as face recognition.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
