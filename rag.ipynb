{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load websites\n",
    "website_urls = [\"https://jayeshmahapatra.github.io/2023/06/22/arcface.html\",\n",
    "                \"https://jayeshmahapatra.github.io/2023/05/28/triton.html\",\n",
    "                \"https://jayeshmahapatra.github.io/2023/12/03/llama2.html\"]\n",
    "loader = WebBaseLoader(website_urls)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n\\n\\n\\n\\nEnhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnhancing Embedding Separation with ArcFace Loss\\n\\nJun 22, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEmbeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.\\nEmbeddings can be obtained from Neural Networks trained with traditional classification losses. However, these losses do not explicitly optimize cosine distances to achieve both inter-class separability and intra-class compactness.\\nIn this article, we will delve into ArcFace loss and its suitability for tasks that require high degree of inter-class separability while still having high intra-class variance, such as face recognition.\\n\\nEmbeddings & Separability\\nBefore we start discussing losses, let’s take a refresher on what embeddings are:\\n\\nEmbeddings are mathematical representations of objects in a vector space.\\nThey can usually be obtained by training a Neural Network.\\nSimilar objects have embeddings that are close to each other.\\nThe similarity is measured using distance metrics like euclidean or cosine distance.\\nWe usually want embeddings such that objects belonging to the same class are close together whereas objects belonging to different classes are further apart.\\n\\n\\n\\n\\nFigure: Visualization of embeddings of audio clips speaking a certain word.  Source: Study of Acoustic Word Embeddings in relation to Mishearing of Words, Jayesh Mahapatra 2021\\n\\n\\n\\n\\nEmbedding separation with Softmax Loss\\nClassical Softmax Loss looks like this\\n\\n\\\\[L_{1} = -log \\\\frac{e^{W^T_{y_i}x_i + b_{y_i}}}{\\\\sum^N_{j=1}e^{W^T_jx_i + b_j}}\\\\]\\n\\nLet’s break down the equation to understand the individual elements:\\n\\nFeatures: $x_i \\\\in \\\\mathbb{R}^d$ represents features of $i$-th example of the $y$-th class.\\nWeights: $W_j \\\\in \\\\mathbb{R}^d$ is the $j$-th column of weight matrix $W \\\\in \\\\mathbb{R}^{d \\\\times N}$.\\nBias: $b_j \\\\in \\\\mathbb{R}^N$ is the bias term.\\nNumber of Classes: $N$ denotes the number of unique classes in the dataset.\\n\\nSince we are not constraining the weight matrix $W$ or the features $x_i$ in anyway, the loss function does not explicitly consider the angular relationship between embeddings. This means that optimizing for this loss doesn’t explicitly promote angular separibility of embeddings belonging to different classes, neither does it explicitly enforce intra-class angular similarity.\\n\\nArcFace Loss\\nLet’s now modify the softmax loss to explicitly care about the angles between embeddings.\\nFor simplicity we can set the bias term $b_j = 0$. \\nThen we can rewrite $W^T_jx_i = \\\\Vert W_j \\\\Vert \\\\Vert x_i \\\\Vert cos \\\\theta_j $ , where $\\\\theta_j$ is the angle between the weight $W_j$ and feature $x_j$.\\nOur goal is to modify the loss function such that the predictions only depend on $\\\\theta_j$.\\nWe can achieve this by normalizing the other two terms in the equation:\\n\\nWe set $\\\\Vert W_j \\\\Vert = 1$ using $l_2$ normalization\\nWe normalize features $ \\\\Vert x_i \\\\Vert $ by $l_2$ normalization and re-scale it by $s$.\\n\\nFollowing these modifications the loss function becomes:\\n\\n\\\\[L_2 = -log \\\\frac{e^{s\\\\cos \\\\theta_{y_i}}}{e^{s\\\\cos \\\\theta_{y_i}} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nNow the predictions only depend on $\\\\theta$, and the features are distributed on a hypersphere with radius $s$. The scaling parameter $s$ is used to control the overall range of output logits which in term affects how large the gradients will be during training.\\nWe can improve this further by introducing an additive angular margin parameter $m$ to the angle $\\\\theta$ between $W_{y_i}$ and $x_i$. A margin encourages better class separation by penalizing the loss function if examples fall within a certain\\ndistance from the decision boundary. In our case, since we are using angular distances, our margin $m$ is also angular.\\nThe final loss then becomes:\\n\\n\\\\[L_3 = -log \\\\frac{e^{s \\\\cos (\\\\theta_{y_i} + m) }}{e^{s\\\\cos (\\\\theta_{y_i} + m)} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nThis is the ArcFace Loss that is widely used in training face recognition systems.\\n\\nExperimental Results\\nIn order to demonstrate the difference in class separation when using arcface loss vs classic softmax, I have created a github repository called ArcFace-Embedding-Visualization.\\nThis repository contains contains code for visualizing how embeddings evolve when trained using ArcFace vs Softmax Loss, as shown below:\\n\\nVisualization of Embedding Separation across Training Epochs\\n\\n\\n\\n\\n\\n            ArcFace 3D embeddings during training.\\n         \\n\\n\\n\\n            Softmax 3D embeddings during training.\\n         \\n\\n\\n\\nAs we can see that the arcface loss results in embedding clusters that are more cleanly separated as well as are more compact than the embeddings trained using classic softmax.\\nThe provided code trains a VGG8 network on the popular MNIST dataset using the ArcFace loss and Softmax loss, and generates visualization of embeddings. For more information on how to create and visualize such embeddings, please checkout the repository.\\nConclusion\\nIn conclusion, in this blog post we introduced ArcFace loss as a powerful technique for achieving enhanced embedding separability. By explicitly considering the angles between embeddings, ArcFace loss provides improved discrimination of embeddings, especially in face recognition tasks.\\nReferences\\n\\nJiankang Deng, , Jia Guo, and Stefanos Zafeiriou. “ArcFace: Additive Angular Margin Loss for Deep Face Recognition”.CoRR abs/1801.07698 (2018).\\n\\n\\n\\n<Previous PostBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\n>Next PostAndroid OCR Translation App using Kotlin and Google ML-Kit\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\nJayesh Mahapatrajayeshmahapatra@gmail.com\\n\\n jayeshmahapatra jayeshmahapatra jayeshmahapatra\\n\\n\\nJayesh's techincal blog where he writes about Machine Learning and Computer Science.\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\nBeyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\nMay 28, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nServing Models\\nIn today’s world, ML engineers are taking on the role of “full stack” professionals, not only developing new models but also deploying them. Python has emerged as the language of choice in the field of Machine Learning, leading engineers to gravitate towards Python-based web frameworks for model deployment. While FastAPI, a modern and high-performance web framework for building APIs in Python, has become a popular choice, it does have certain limitations when it comes to serving ML models effectively.\\n\\nThe Limitations of FastAPI\\nFastAPI, with its support for asynchronous request processing and data validation, empowers ML engineers to write REST API endpoints for ML inference. However, it wasn’t explicitly designed to serve as a dedicated inference engine. As a result, it lacks built-in support for ML frameworks and inference-specific features that can maximize throughput. Although FastAPI is ok for scenarios where high throughput isn’t critical, as ML inference scales, it becomes advisable to transition to an inference engine that is explicitly built for serving ML models to maximize performance and efficiency.\\nIntroducing Nvidia Triton: A Powerful Inference Serving Platform\\nNvidia Triton is an open-source inference serving platform that excels in optimizing and scaling machine learning model deployment. It surpasses FastAPI in several key aspects, making it an ideal solution for serving ML models efficiently:\\n\\n\\nFramework Support: Triton supports all major ML frameworks, including Tensorflow, PyTorch, TensorRT, Onnx, and more. This built-in framework support simplifies the deployment process for models developed using various ML frameworks.\\n\\n\\nDynamic Batching: Triton can batch incoming requests in an online fashion, maximizing throughput and reducing latency. This efficient handling of multiple inference requests simultaneously enhances performance.\\n\\n\\nConcurrent Model Execution: Triton enables loading multiple models and instances into GPU memory. These models can be executed in parallel, further boosting throughput and performance.\\n\\n\\nGPU Accelerated Preprocessing: Triton leverages the power of GPUs with the NVIDIA Data Loading Library (DALI) for faster data transformation and enhanced inference speed.\\n\\n\\nIntegration with Kubernetes and Prometheus: Triton seamlessly integrates with Kubernetes for efficient container orchestration, enabling scalable deployment and management of ML models. Its integration with Prometheus facilitates monitoring and observability, essential for ML operations.\\n\\n\\nHowever, triton does have some drawbacks:\\n\\nLarge Docker Image Size: Triton’s Docker images can be large (~10.6 GB), even when deploying a Triton server with specific backends. This can result in increased storage and network requirements.\\nLearning Curve: Setting up and configuring the Triton server may require some additional time and effort due to its advanced features and flexibility.\\n\\nWhile these disadvantages should be taken into account, they are outweighed by the benefits when high throughput, scalability, and efficiency are the priority.\\nTo learn more about Nvidia Triton, you can visit the official website.\\n\\nExample Repository: Deploying PyTorch Model for Bee vs. Ant Classification with Triton\\nTo demonstrate the deployment workflow of ML models using NVIDIA Triton, FastAPI, and Gradio, I have created the example repository called triton-fastapi-docker. This repository provides a practical implementation of utilizing Triton with Docker containers to deploy a PyTorch-based machine learning model capable of distinguishing between pictures of bees and ants.\\nThe triton-fastapi-docker repository comprises the following services:\\n\\ntriton: This service runs the NVIDIA Triton inference engine, serving the PyTorch ML model that distinguishes between pictures of bees and ants.\\nfastapi: The Python backend communicates with triton service via gRPC, handling client requests effectively.\\ngradio: This service provides a user-friendly interface using Gradio, allowing users to upload images for inference and interact with the inference pipeline seamlessly.\\n\\nBy following the instructions provided in the repository’s README, users can easily set up and deploy their own PyTorch-based ML model. Furthermore, the repository includes a Jupyter notebook located in the notebooks directory, offering a step-by-step guide on creating a TorchScript model specifically designed to distinguish between pictures of bees and ants.\\nI hope this blog helped you consider triton for your future model deployments, and the repo serves as a valuable example for the same !\\n\\n\\n<Previous PostHello World 👋\\n\\n>Next PostEnhancing Embedding Separation with ArcFace Loss\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\nJayesh Mahapatrajayeshmahapatra@gmail.com\\n\\n jayeshmahapatra jayeshmahapatra jayeshmahapatra\\n\\n\\nJayesh's techincal blog where he writes about Machine Learning and Computer Science.\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/05/28/triton.html', 'title': 'Beyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog', 'description': 'Serving Models', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nllama2.npy : Implementing Llama2 LLM using just Python and Numpy | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh\\'s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nllama2.npy : Implementing Llama2 LLM using just Python and Numpy\\n\\nDec 3, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLarge Language Models (LLMs), such as GPT-4, Claude, and Llama2, have reshaped the landscape of Natural Language Processing (NLP), democratizing AI applications. These models often have billions of parameters and are trained on massive datasets of text, often crawled from the internet.\\nRecently Andrej Karpathy released a github repository called llama2.c, in which he implements the architecture of Llama2 LLM at various scales, and has a custom built inference code written completely in C. Taking inspiration from that, contributors have created/ported their own Llama2 inference code to different languages.\\nIn this blog post, I will cover my own implementation in github called llama2.npy where I port llama2.c to do inference using only Python and Numpy. I will go through the basics of the Llama2 architecture and the main modules of the inference pipeline like the tokenizer, attention, positional embeddings and the text generator.\\n\\n\\n\\nExample of generating text from the custom Llama implementation.\\n\\n\\n\\n\\nLlama2 Architecture\\nByte Pair Encoding Tokenizer\\nAttention\\nRotary Position Embedding (RoPE)\\nText Generation\\n\\nMost of the porting was straightforward due to similarities of the PyTorch and Numpy APIs, but there needed to be some custom implementation done for incompatible functionalities, Neural Network Layers and Normalizers. Further, some work was needed to implement a custom tokenizer and model weight loading functionalities. Throughout the process, I also simpliflied the code where needed.\\nLlama2 Architecture\\nThe Llama2 architecture is portrayed in the figure below:\\n\\n\\n\\n High level diagram of Llama2 Architecture.\\n\\n\\n\\nThe Llama2 architecture is fairly simple and modular and contains the following modules:\\n\\nTokenization: The input text is first tokenized, then converted to unique representations (ids) using a Byte Pair Encoding (BPE) tokenizer.\\nToken Embedding: The token ids are then fed into a token embedder to get token embeddings, which are then passed through a dropout layer to get token embeddings.\\nTransformer Blocks: The token embeddings are then fed into layers of Transformer Blocks which consist of RMS Norms,Attention, Rotary Positional Embeddings and Feed Forwards Networks.\\nOutput Projection: The final output from the transformer blocks is fed through a RMS Norm layer into a linear layer to get the output.\\nAutoregressive Sampling: To get a sentence completion we autoregressivley feed the network’s output into itself to keep generating new tokens.\\n\\nImplementation Specific Deviations:\\n\\nNo KV-Caching: In this implementation we do naive sampling without any KV-caching, which is inefficient in practice.\\nWeight Tying: The weights of the token embedder and the output linear layer are shared. This is for increased efficiency and better convergence of our smaller model.\\n\\nNow let’s explore the important parts of this architecture one by one.\\nByte Pair Encoding Tokenizer\\nNeural Networks operate on numerical data, they multiply numbers. But then how do language models like Llama2 or GPT understand the text we write ?\\nThe key lies in the use of Tokenizers, specialized modules that convert text into a set of tokens, that then we can represent using numerical ids for processing by NLP models.\\nAn ideal tokenizer would convert text into unique ids while keeping the possible number of representations small and meaningful.\\nThe most popular approaches to tokenization involve:\\n\\nCharacter based tokenizers: Map each character in the vocabulary to an unique representation.\\nWord based tokenizers: Map each individual word in the vocabulary to an unique representation.\\nSubword based tokenizers: Create a vocabulary of subwords using some method. Split words into subwords that are then mapped to unique representations.\\n\\nLlama2 relies on one such subword tokenization method called Byte-Pair Encoding (BPE).\\nTo create a subword vocabulary from a given string of text using Byte-Pair encoding, we follow the following steps:\\n\\nInitialize: Start with a vocabulary of individual characters or subwords.\\nTokenize: Break the text into characters or subwords.\\nCount pairs: Count the frequency of pairs of consecutive characters or subwords.\\nMerge: Merge the most frequent pair into a single token and update the vocabulary.\\nRepeat: Iterate the above steps until the desired vocabulary size is reached or no more merges can be made.\\n\\nNow, for Llama2, such a vocabulary is already created, so we can just download and use that. The vocabulary contains a mapping from bytestrings to token_ids as well as scores for each token_id representing how frequent they were in the training set.\\nThe only thing we need to write is a Python class with the logic to:\\n\\nEncode a given text into token_ids, using the saved vocabulary\\nDecode a list of token ids into text\\n\\nBelow is the Python class I implemented to these:\\n# Create a Tokenizer class that will be used to tokenize the input text\\nimport struct\\nimport sys\\nimport numpy as np\\n\\nclass Tokenizer():\\n\\n    def __init__(self, model_path: str, vocab_size = 32000) -> None:\\n\\n        self.vocab_size = vocab_size\\n\\n        # Load the tokenizer, store in dicts for fast lookup\\n        self.vocab2index, self.vocab_score2index = self._load_tokenizer(model_path)\\n        self.index2vocab = {v: k for k, v in self.vocab2index.items()}\\n        self.index2vocab_score = {v: k for k, v in self.vocab_score2index.items()}\\n\\n    # An internal function called _load_tokenizer, takes str input, outputs a tuple (dict, dict, int)\\n    def _load_tokenizer(self, model_path: str) -> tuple:\\n\\n        max_token_length = 0\\n        self.vocab2index = {}\\n        self.vocab_score2index = {}\\n        \\n\\n        with open(model_path, \\'rb\\') as file:\\n\\n            max_token_length = struct.unpack(\\'i\\', file.read(4))[0]\\n\\n            for i in range(0, self.vocab_size):\\n\\n                score = struct.unpack(\\'f\\', file.read(4))[0]\\n                str_len = struct.unpack(\\'i\\', file.read(4))[0]\\n                bytestr = file.read(str_len)\\n\\n                if type(bytestr) is not str:\\n                    bytestr = bytestr.decode(\\'utf8\\')\\n\\n                self.vocab2index[bytestr] = i\\n                self.vocab_score2index[score] = i\\n\\n        return self.vocab2index, self.vocab_score2index\\n    \\n\\n    def encode(self, initial_string: str, bos: bool, eos: bool) -> list:\\n\\n        # Encode the initial string character by character, assunes all characters are in vocab\\n        tokens = [self.vocab2index[char] for char in initial_string]\\n        \\n\\n        # Merge consecutive pairs of tokens based on vocab_scores, stop when merging no longer increases the score\\n        while True:\\n            best_score = np.NINF\\n            best_id = -1\\n            best_idx = -1\\n\\n            # Iterate over all consecutive pairs of tokens\\n            for i in range(len(tokens) - 1):\\n\\n                # Convert the pair of tokens into a string\\n                string = self.index2vocab[tokens[i]] + self.index2vocab[tokens[i + 1]]\\n                \\n                # Get the ID of this merged string in vocab\\n                str_id = self.vocab2index.get(string, None)\\n\\n                if str_id is not None:\\n\\n                    if self.index2vocab_score[str_id] > best_score:\\n                        # We found a better pair to merge\\n                        best_score = self.index2vocab_score[str_id]\\n                        best_id = str_id\\n                        best_idx = i\\n\\n            if best_idx == -1:\\n                break  # We couldn\\'t find any more pairs to merge, so we\\'re done\\n\\n            # Merge the consecutive pair (best_idx, best_idx+1)\\n\\n            # Replace token at position best_idx with best_id of the merged pair\\n            tokens[best_idx] = best_id\\n\\n            # Delete token at position best_idx+1\\n            tokens = tokens[0:best_idx + 1] + tokens[best_idx + 2:]\\n\\n        return tokens\\n\\n    def decode(self, pt_tokens: list) -> str:\\n\\n        # Convert the list of token IDs back into a string\\n        text = \\'\\'.join([self.index2vocab[token] for token in pt_tokens])\\n\\n        return text\\n            \\n\\nIn this code the Tokenizer class is desinged for BPE tokenization. It includes methods for loading the tokenizer vocabulary (with scores), encoding a given string, and decoding a list of token IDs back into the original text.\\nThe encoding processing is similar to the BPE encoding described above, with the difference being instead of calculating token frequency at each iteration, we get token scores from the saved vocbalary. Based on these scores, we decide whether to merge subwords a pair of subwords and represent them using a single token or keep using separate tokens for them.\\nTransformer Block\\nNow once we have token ids we forward them through standard text embedding and dropout layers to get text embeddings.\\nThese are then passed through multiple transformer blocks.\\nA transformer block has the following architecture:\\n\\n\\n\\n High level diagram of Llama2 Transformer Blocks.\\n\\n\\n\\nA transformer block passes the input through an multi headed self attention layer followed by an feed forward layer both with RMS norms and skip connections. The feed forward layer has SiLU activations. The attention module also incorporates Rotary Positional Embedding to take into account position of tokens in a text during attention.\\nAttention\\nAttention is a mechanism for our model to incorporate influence of other related tokens in a sequence when encoding a specific token. Self attention is simply the case where the input and output sequences are the same.\\nFor e.g. in the sentence “Sam has a big house”, “big” is associated more with “house” than with “Sam”.\\nTo learn about Attention, let’s first start with the simpler single-headed attention, and then we will move to discuss the multi-headed variant.\\nSingle Headed Attention\\nWe calculate a single headed self-attention output for a sequence by the following steps:\\n\\n\\nEmbed and Pack: Calculate emebdding $x_i$ for each token at $i$-th position in the sequence, and pack them together to get the embedded matrix $X$.\\n\\n\\nCalculate Query, Key, Value Matrices: Calculate Query, Key, Value matrices $Q$, $K$ and $V$ by the following equations: \\\\(Q = X \\\\times W^Q\\\\)  \\\\(K = X \\\\times W^K\\\\)  \\\\(V = X \\\\times W^V\\\\) \\n\\n\\nCalculate Scores and apply causal mask: We can get a score matrix containing scores for each token with respect to other tokens by using the equation:  \\\\(S = Q \\\\times K^T\\\\) \\nHowever, this would mean tokens are also influenced by tokens occuring after them in the sequence. If we want a causal model where the model only calculates the next token based on previous tokens, we can add an upper triangle mask to the score matrix.  \\\\(S_{masked} = S + Mask\\\\)  where $Mask$ is a upper triangle matrix with $0s$ in valid places and negative infinity in places we don’t want to attend. These negative infinities will resolve to $0s$ after $softmax$ in the next step.\\n\\n\\nCalculate Output Encodings: We get the final encoding for each token using the equation  \\\\(Z = softmax(\\\\frac{S_{masked}}{\\\\sqrt{d_k}}) \\\\times V\\\\)  where $i$-th row contains encoding for $i$-th token, and $d_k$ is the dimension of the key vector.\\n\\n\\nEssentially these matrices represent the following:\\n\\nQuery: What the tokens are looking for in other tokens.\\nKey: What these token offer as influence to other tokens.\\nValue: What these tokens actually contain.\\n\\nThe weight matrices $W^{Q}$, $W^{K}$, $W^{V}$ are learned during the model training.\\nMulti-Headed Attention\\nIn the multi headed case, we just maintain multiple weight matrices $ W_{i}^{Q} , W_{i}^{K}, W_{i}^{V} $ \\nand hence for each attention block, we can go through the attention process multiple times with different weight matrices. This way we generate multiple encodings $ Z_{i} $ where $i$ goes from $0$ to $n - 1$ with $n$ being the number of heads.\\nAt the end we get the final encoding matrix $Z$ for the sequence by just concatenating all the $Z_i$ matrices and multiplying them with a matrix $W^0$.\\nBelow is the python class implementing Multi-headed attention:\\nclass Attention():\\n    def __init__(self, args: ModelArgs):\\n        super().__init__()\\n\\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\\n        self.n_local_heads = args.n_heads\\n        self.n_local_kv_heads = self.n_kv_heads\\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\\n        self.head_dim = args.dim // args.n_heads\\n        \\n        self.wq = NumpyLinear(args.dim, args.n_heads * self.head_dim, bias=False)\\n        self.wk = NumpyLinear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\\n        self.wv = NumpyLinear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\\n        self.wo = NumpyLinear(args.n_heads * self.head_dim, args.dim, bias=False) \\n        \\n        self.attn_dropout = NumpyDropout(args.dropout) \\n        self.resid_dropout = NumpyDropout(args.dropout) \\n        self.dropout = args.dropout\\n\\n        \\n        # create causal attention mask\\n        mask = np.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\")).astype(np.float32)\\n        mask = np.triu(mask, k=1).astype(np.float32)\\n        self.mask = mask\\n\\n    def forward(\\n        self,\\n        x: np.ndarray,\\n        freqs_cos: np.ndarray,\\n        freqs_sin: np.ndarray,\\n    ):\\n        bsz, seqlen, _ = x.shape\\n\\n        # QKV\\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\\n        xq = xq.reshape(bsz, seqlen, self.n_local_heads, self.head_dim)\\n        xk = xk.reshape(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\\n        xv = xv.reshape(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\\n\\n        # RoPE relative positional embeddings\\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\\n\\n        # grouped multiquery attention: expand out keys and values\\n        xk = repeat_kv(xk, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\\n        xv = repeat_kv(xv, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\\n\\n        # make heads into a batch dimension\\n        xq = np.transpose(xq, (0,2,1,3)) # (bs, n_local_heads, seqlen, head_dim)\\n        xk = np.transpose(xk, (0,2,1,3))\\n        xv = np.transpose(xv, (0,2,1,3))\\n\\n        # manual implementation\\n        scores = np.matmul(xq, np.transpose(xk, (0,1,3,2))) / np.sqrt(self.head_dim)\\n        scores = scores + self.mask[:, :, :seqlen, :seqlen]   # (bs, n_local_heads, seqlen, cache_len + seqlen)\\n        scores = numpy_softmax(scores, axis=-1)\\n        scores = self.attn_dropout(scores)\\n        output = np.matmul(scores, xv)  # (bs, n_local_heads, seqlen, head_dim)\\n\\n        # restore time as batch dimension and concat heads\\n        output = np.transpose(output, (0,2,1,3)).reshape(bsz, seqlen, -1)\\n\\n        # final projection into the residual stream\\n        output = self.wo(output)\\n        output = self.resid_dropout(output)\\n        return output\\n\\nIn this code we:\\n\\nCalculate the Query, Key, Value matrices\\nApply RoPE embedding\\nCalculate scores by using Query and Keys, and apply causal mask and dropout.\\nCalculate encoded matrices $Z_i$ using scores and the head dimension.\\nCalculate final output $Z$ by multiplying concatenated $Z_i$ matrices with weight $W_0$\\nReturn the final output after passing through a dropout.\\n\\nIf you want to learn more about attention and how it works in detail, I would recommend checking out the excellent The Illustrated Transformer blog post by Jay Alammar.\\nRotary Position Embedding (RoPE)\\nAttention as we saw above allows token encodings to be influenced by other tokens. However, this influence as we discussed above doesn’t take into account the position of the influencing tokens. Ideally for e.g. tokens far away in the sequence should have different level of influence than tokens right next to each other.\\nThis kind of additional information can be added to the token embeddings before or during the attention process, by using positional embeddings. Llama 2 uses one such positional embedding called Rotary Positional Embedding (RoPE). It was first introduced by Su et al[5] in the RoFormer paper.\\nSpecifically, given two token embeddings $x_i$ and $x_j$, we do RoPE embedding by:\\n\\n\\nGetting the query, key vectors $q_i$ and $k_j$ from them using weight matrices.\\n\\n\\nThen encoding the positional information between them by rotating both the vectors by using a predefined rotation matrix $R^d_{\\\\Theta,m}$ where $m$ is the position of the token being encoded, $\\\\Theta \\\\in \\\\mathbb{R}^{d/2}$ is a preset constant defined as $\\\\Theta = \\\\{ \\\\theta_i = {10000}^{-2(i-1)/d}, i \\\\in [1,2, \\\\dots, d/2] \\\\} $, and $d$ is the dimension of the key, query vectors. The rotation matrix $R^d_{\\\\Theta,m}$ has the formulation: \\\\(R^d_{\\\\theta,m}x = \\n{\\\\begin{pmatrix} x_1 \\\\\\\\ x_2 \\\\\\\\x_3 \\\\\\\\ x_4 \\\\\\\\ \\\\vdots \\\\\\\\ x_{d-1} \\\\\\\\ x_d \\\\end{pmatrix}} \\\\otimes \\n{\\\\begin{pmatrix} \\\\cos m \\\\theta_1  \\\\\\\\ \\\\cos m \\\\theta_1 \\\\\\\\ \\\\cos m \\\\theta_2 \\\\\\\\ \\\\cos m \\\\theta_2  \\\\\\\\ \\\\vdots \\\\\\\\ \\\\cos m \\\\theta_{d/2} \\\\\\\\ \\\\cos m \\\\theta_{d/2} \\\\end{pmatrix}} + \\n{\\\\begin{pmatrix} -x_2 \\\\\\\\ x_1 \\\\\\\\ -x_4 \\\\\\\\ x_3 \\\\\\\\ \\\\vdots \\\\\\\\ -x_d \\\\\\\\ x_{d-1} \\\\end{pmatrix}} \\\\otimes \\n{\\\\begin{pmatrix}  \\\\sin m \\\\theta_1  \\\\\\\\ \\\\sin m \\\\theta_1 \\\\\\\\ \\\\sin m \\\\theta_2 \\\\\\\\ \\\\sin m \\\\theta_2  \\\\\\\\ \\\\vdots \\\\\\\\ \\\\sin m \\\\theta_{d/2} \\\\\\\\ \\\\sin m \\\\theta_{d/2} \\\\end{pmatrix}}\\\\) \\n Essentially for the key, query vectors we rotate their elements two at a time.\\n\\n\\nAfter this applying the softmax calculation and multiplying with value matrix as normal.\\n\\n\\nBelow is the python implementation of RoPE embeddings\\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\\n    freqs = 1.0 / (theta ** (np.arange(0, dim, 2)[: (dim // 2)].astype(np.float32) / dim))\\n    t = np.arange(end).astype(np.float32)\\n    freqs = np.outer(t, freqs).astype(np.float32)\\n    freqs_cos = np.cos(freqs)\\n    freqs_sin = np.sin(freqs)\\n    return freqs_cos, freqs_sin\\n\\ndef apply_rotary_emb(\\n    xq: np.ndarray,\\n    xk: np.ndarray,\\n    freqs_cos: np.ndarray,\\n    freqs_sin: np.ndarray,\\n) -> Tuple[np.ndarray, np.ndarray]:\\n\\n    # reshape xq and xk to match the complex representation\\n    xq_reshaped = xq.astype(np.float32).reshape(xq.shape[:-1] + (-1, 2))\\n    xq_r = xq_reshaped[..., 0]\\n    xq_i = xq_reshaped[..., 1]\\n    \\n    xk_reshaped = xk.astype(np.float32).reshape(xk.shape[:-1] + (-1, 2))\\n    xk_r = xk_reshaped[..., 0]\\n    xk_i = xk_reshaped[..., 1]\\n\\n    # reshape freqs_cos and freqs_sin for broadcasting\\n    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\\n    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\\n\\n    # apply rotation using real numbers, this is similar to rotating a vector theta degrees using the euler notation\\n    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin # Real part of rotated vectors\\n    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos # Imaginary part of rotated vectors\\n    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin # Real part of rotated vectors\\n    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos # Imaginary part of rotated vectors\\n\\n    # flatten the last two dimensions\\n    xq_out = np.stack([xq_out_r, xq_out_i], axis=-1).reshape(xq.shape[:3] + (-1,))\\n    xk_out = np.stack([xk_out_r, xk_out_i], axis=-1).reshape(xk.shape[:3] + (-1,))\\n\\n    return xq_out.astype(xq.dtype), xk_out.astype(xk.dtype)\\n\\nIn this implementation, we do RoPE encoding in the following steps:\\n\\n\\nPrecompute the Cos and Sin vectors: We precompute the $\\\\cos$ and $\\\\sin$ vectors in the $R^d_{\\\\Theta,m}$ formulation for all positions in dimension $d$.\\n\\nReshape Key, Query vectors: Represent the key and query vectors as complex numbers by reshaping them.\\nRotate Key, Query vectors: Rotate the key, query vectors by applying rotation in the euler form.\\nFlatten and return: Flatten the rotated vectors so that they regain their original shape and return.\\n\\nText Generation\\nOnce the network is built and trained, we need to be able to prompt it to generate text. This is usually done by\\ninputing an initial text snippet to the network, and then autoregressively generating new tokens.\\nTo generate n tokens given a sequence we do the following:\\n\\nLogits for next token: Pass the initial prompt + any tokens generated till now to the model, and get model logits for the next token.\\n\\nSample new token: Based on the logits, we pick the next token by sampling. There two ways we can go about this:\\n2.1. No temperature: If no temperature parameter is given for the generation, use the logits as probabilities, and sample the next token\\n2.2 Temperate and Top_K: Divide logits by the temperature. Then (optionally) filter to only the top_k token indices by value for more efficient sampling. Finally, convert the rescaled logits to probabilities using softmax and sample.\\n\\nAppend and Repeat: Append the new token to the sequence and repeat steps 1-3 until the number of tokens generated equals n.\\n\\nBelow is the python implementation of text generation function:\\n  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\\n        \"\"\"\\n        Autoregressively feed the model the promt + generated tokens at each step.\\n        This is a naive implementation without Key, Value cache.\\n        \"\"\"\\n        for _ in range(max_new_tokens):\\n            # if the sequence context is growing too long we must crop it at block_size\\n            idx_cond = idx if idx.size <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\\n            # forward the model to get the logits for the index in the sequence\\n            logits = self(idx_cond)\\n            logits = logits[:, -1, :] # crop to just the final time step\\n            if temperature == 0.0:\\n                # \"sample\" the single most likely index\\n                _, idx_next = numpy_topk_by_partition(logits, k=1, axis=-1)\\n            else:\\n                # pluck the logits at the final step and scale by desired temperature\\n                logits = logits / temperature\\n                # optionally crop the logits to only the top k options\\n                if top_k is not None:\\n\\n                    v, _ = numpy_topk_by_partition(logits, k=min(top_k, logits.size(-1)), axis=-1)\\n                    logits[logits < v[:, [-1]]] = -float(\\'Inf\\')\\n                # apply softmax to convert logits to (normalized) probabilities\\n                probs = numpy_softmax(logits, axis=-1)\\n                # sample from the distribution\\n                idx_next = np.random.choice(self.params.vocab_size, p=probs.squeeze())\\n            # append sampled index to the running sequence and continue\\n            idx = np.concatenate((idx, np.array([idx_next]).reshape(1,-1)), axis=1)\\n\\n        return idx\\n\\nIn this generate function we are given an initial prompt and the number of tokens we want to generate.\\nThen we do the following:\\n\\nPass the initial prompt + any tokens generated till now to the model, and get model logits\\nBased on the logits, sample the next token to be generated. Depending on the whether temperature and top_k are given, we choose whether to rescale logits and whether to only sample from the top_k logits instead of entire vocabulary.\\nConcatenate the new token to the sequence and repeat till we have generated max_new_tokens number of tokens.\\n\\nFeatures missing from this implementation\\nFor simplicity we omitted some features in this implementation:\\n\\nKV-Cache: KV-Cache allows for caching during auto-regressive text generation, increasing compute efficiency significantly, you can learn more about it in the video here.\\nGrouped Query Attention: This is a modification of multi-headed attention where for computational efficiency we share key and value heads across multiple query heads. You can check out this paper by Ainslie et al[6] for more details.\\n\\nConclusion\\nIn this blog post we explored the architecture of Llama2, and learned how to implement key functionalities from scratch.\\nWe can see that in practice the architecture is a simple mixture of transformer blocks with regularizers and positional embeddings.\\nYou can check out the complete source code at my github repo llama2.npy and run it locally by following the instructions.\\nReferences\\n\\nTouvron et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models” (2023)\\nAndrej Karpathy, llama2.c (2023)\\nVaswani et al. “Attention is All You Need”, CoRR abs/1706.03762 (2017)\\nJay Alamar, “The Illustrated Transformer” (2018)\\nSu et al. “RoFormer: Enhanced Transformer with Rotary Position Embedding”, CoRR abs/2104.09864 (2021)\\nAinslie et al. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints”, eprint 2305.13245 (2023)\\n\\n\\n\\n<Previous PostAndroid OCR Translation App using Kotlin and Google ML-Kit\\n\\n>Blog ArchiveArchive of all previous blog posts\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh\\'s Blog\\n\\n\\n\\nJayesh Mahapatrajayeshmahapatra@gmail.com\\n\\n jayeshmahapatra jayeshmahapatra jayeshmahapatra\\n\\n\\nJayesh\\'s techincal blog where he writes about Machine Learning and Computer Science.\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://jayeshmahapatra.github.io/2023/12/03/llama2.html', 'title': 'llama2.npy : Implementing Llama2 LLM using just Python and Numpy | Jayesh’s Blog', 'description': 'Large Language Models (LLMs), such as GPT-4, Claude, and Llama2, have reshaped the landscape of Natural Language Processing (NLP), democratizing AI applications. These models often have billions of parameters and are trained on massive datasets of text, often crawled from the internet.', 'language': 'en'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = [\"\\n\\n\", \"\\n\", \"\\\\[\", \"//]\", \"\\\\(\", '\\\\)',  \" \", \"\"]\n",
    "chunk_size = 256\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=int(chunk_size/10),\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=separators,\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3917"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the chunks\n",
    "len(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"mistral:instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"<s> [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. [/INST] </s> \n",
    "[INST] Question: {input} \n",
    "Context: {context} \n",
    "Answer: [/INST]\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = retrieval_chain.invoke({\"input\": \"Why use arcface loss?\"})\n",
    "# print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:...\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Why use arcface loss?'}\n",
      "{'context': [Document(page_content=\"\\\\[L_3 = -log \\\\frac{e^{s \\\\cos (\\\\theta_{y_i} + m) }}{e^{s\\\\cos (\\\\theta_{y_i} + m)} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nThis is the ArcFace Loss that is widely used in training face recognition systems.\\n\\nExperimental Results\\nIn order to demonstrate the difference in class separation when using arcface loss vs classic softmax, I have created a github repository called ArcFace-Embedding-Visualization.\\nThis repository contains contains code for visualizing how embeddings evolve when trained using ArcFace vs Softmax Loss, as shown below:\\n\\nVisualization of Embedding Separation across Training Epochs\\n\\n\\n\\n\\n\\n            ArcFace 3D embeddings during training.\\n         \\n\\n\\n\\n            Softmax 3D embeddings during training.\\n         \\n\\n\\n\\nAs we can see that the arcface loss results in embedding clusters that are more cleanly separated as well as are more compact than the embeddings trained using classic softmax.\\nThe provided code trains a VGG8 network on the popular MNIST dataset using the ArcFace loss and Softmax loss, and generates visualization of embeddings. For more information on how to create and visualize such embeddings, please checkout the repository.\\nConclusion\\nIn conclusion, in this blog post we introduced ArcFace loss as a powerful technique for achieving enhanced embedding separability. By explicitly considering the angles between embeddings, ArcFace loss provides improved discrimination of embeddings, especially in face recognition tasks.\\nReferences\\n\\nJiankang Deng, , Jia Guo, and Stefanos Zafeiriou. “ArcFace: Additive Angular Margin Loss for Deep Face Recognition”.CoRR abs/1801.07698 (2018).\\n\\n\\n\\n<Previous PostBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\n>Next PostAndroid OCR Translation App using Kotlin and Google ML-Kit\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\nJayesh Mahapatrajayeshmahapatra@gmail.com\\n\\n jayeshmahapatra jayeshmahapatra jayeshmahapatra\\n\\n\\nJayesh's techincal blog where he writes about Machine Learning and Computer Science.\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en'}), Document(page_content=\"Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnhancing Embedding Separation with ArcFace Loss\\n\\nJun 22, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEmbeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.\\nEmbeddings can be obtained from Neural Networks trained with traditional classification losses. However, these losses do not explicitly optimize cosine distances to achieve both inter-class separability and intra-class compactness.\\nIn this article, we will delve into ArcFace loss and its suitability for tasks that require high degree of inter-class separability while still having high intra-class variance, such as face recognition.\\n\\nEmbeddings & Separability\\nBefore we start discussing losses, let’s take a refresher on what embeddings are:\\n\\nEmbeddings are mathematical representations of objects in a vector space.\\nThey can usually be obtained by training a Neural Network.\\nSimilar objects have embeddings that are close to each other.\\nThe similarity is measured using distance metrics like euclidean or cosine distance.\\nWe usually want embeddings such that objects belonging to the same class are close together whereas objects belonging to different classes are further apart.\\n\\n\\n\\n\\nFigure: Visualization of embeddings of audio clips speaking a certain word.  Source: Study of Acoustic Word Embeddings in relation to Mishearing of Words, Jayesh Mahapatra 2021\\n\\n\\n\\n\\nEmbedding separation with Softmax Loss\\nClassical Softmax Loss looks like this\\n\\n\\\\[L_{1} = -log \\\\frac{e^{W^T_{y_i}x_i + b_{y_i}}}{\\\\sum^N_{j=1}e^{W^T_jx_i + b_j}}\\\\]\\n\\nLet’s break down the equation to understand the individual elements:\\n\\nFeatures: $x_i \\\\in \\\\mathbb{R}^d$ represents features of $i$-th example of the $y$-th class.\\nWeights: $W_j \\\\in \\\\mathbb{R}^d$ is the $j$-th column of weight matrix $W \\\\in \\\\mathbb{R}^{d \\\\times N}$.\\nBias: $b_j \\\\in \\\\mathbb{R}^N$ is the bias term.\\nNumber of Classes: $N$ denotes the number of unique classes in the dataset.\\n\\nSince we are not constraining the weight matrix $W$ or the features $x_i$ in anyway, the loss function does not explicitly consider the angular relationship between embeddings. This means that optimizing for this loss doesn’t explicitly promote angular separibility of embeddings belonging to different classes, neither does it explicitly enforce intra-class angular similarity.\\n\\nArcFace Loss\\nLet’s now modify the softmax loss to explicitly care about the angles between embeddings.\\nFor simplicity we can set the bias term $b_j = 0$. \\nThen we can rewrite $W^T_jx_i = \\\\Vert W_j \\\\Vert \\\\Vert x_i \\\\Vert cos \\\\theta_j $ , where $\\\\theta_j$ is the angle between the weight $W_j$ and feature $x_j$.\\nOur goal is to modify the loss function such that the predictions only depend on $\\\\theta_j$.\\nWe can achieve this by normalizing the other two terms in the equation:\\n\\nWe set $\\\\Vert W_j \\\\Vert = 1$ using $l_2$ normalization\\nWe normalize features $ \\\\Vert x_i \\\\Vert $ by $l_2$ normalization and re-scale it by $s$.\\n\\nFollowing these modifications the loss function becomes:\\n\\n\\\\[L_2 = -log \\\\frac{e^{s\\\\cos \\\\theta_{y_i}}}{e^{s\\\\cos \\\\theta_{y_i}} + \\\\sum^{N}_{j=1,j\\\\neq y_i} e^{s \\\\cos \\\\theta_j}}\\\\]\\n\\nNow the predictions only depend on $\\\\theta$, and the features are distributed on a hypersphere with radius $s$. The scaling parameter $s$ is used to control the overall range of output logits which in term affects how large the gradients will be during training.\\nWe can improve this further by introducing an additive angular margin parameter $m$ to the angle $\\\\theta$ between $W_{y_i}$ and $x_i$. A margin encourages better class separation by penalizing the loss function if examples fall within a certain\\ndistance from the decision boundary. In our case, since we are using angular distances, our margin $m$ is also angular.\\nThe final loss then becomes:\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh’s Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en'}), Document(page_content=\"Beyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\nMay 28, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nServing Models\\nIn today’s world, ML engineers are taking on the role of “full stack” professionals, not only developing new models but also deploying them. Python has emerged as the language of choice in the field of Machine Learning, leading engineers to gravitate towards Python-based web frameworks for model deployment. While FastAPI, a modern and high-performance web framework for building APIs in Python, has become a popular choice, it does have certain limitations when it comes to serving ML models effectively.\\n\\nThe Limitations of FastAPI\\nFastAPI, with its support for asynchronous request processing and data validation, empowers ML engineers to write REST API endpoints for ML inference. However, it wasn’t explicitly designed to serve as a dedicated inference engine. As a result, it lacks built-in support for ML frameworks and inference-specific features that can maximize throughput. Although FastAPI is ok for scenarios where high throughput isn’t critical, as ML inference scales, it becomes advisable to transition to an inference engine that is explicitly built for serving ML models to maximize performance and efficiency.\\nIntroducing Nvidia Triton: A Powerful Inference Serving Platform\\nNvidia Triton is an open-source inference serving platform that excels in optimizing and scaling machine learning model deployment. It surpasses FastAPI in several key aspects, making it an ideal solution for serving ML models efficiently:\\n\\n\\nFramework Support: Triton supports all major ML frameworks, including Tensorflow, PyTorch, TensorRT, Onnx, and more. This built-in framework support simplifies the deployment process for models developed using various ML frameworks.\\n\\n\\nDynamic Batching: Triton can batch incoming requests in an online fashion, maximizing throughput and reducing latency. This efficient handling of multiple inference requests simultaneously enhances performance.\\n\\n\\nConcurrent Model Execution: Triton enables loading multiple models and instances into GPU memory. These models can be executed in parallel, further boosting throughput and performance.\\n\\n\\nGPU Accelerated Preprocessing: Triton leverages the power of GPUs with the NVIDIA Data Loading Library (DALI) for faster data transformation and enhanced inference speed.\\n\\n\\nIntegration with Kubernetes and Prometheus: Triton seamlessly integrates with Kubernetes for efficient container orchestration, enabling scalable deployment and management of ML models. Its integration with Prometheus facilitates monitoring and observability, essential for ML operations.\\n\\n\\nHowever, triton does have some drawbacks:\\n\\nLarge Docker Image Size: Triton’s Docker images can be large (~10.6 GB), even when deploying a Triton server with specific backends. This can result in increased storage and network requirements.\\nLearning Curve: Setting up and configuring the Triton server may require some additional time and effort due to its advanced features and flexibility.\\n\\nWhile these disadvantages should be taken into account, they are outweighed by the benefits when high throughput, scalability, and efficiency are the priority.\\nTo learn more about Nvidia Triton, you can visit the official website.\\n\\nExample Repository: Deploying PyTorch Model for Bee vs. Ant Classification with Triton\\nTo demonstrate the deployment workflow of ML models using NVIDIA Triton, FastAPI, and Gradio, I have created the example repository called triton-fastapi-docker. This repository provides a practical implementation of utilizing Triton with Docker containers to deploy a PyTorch-based machine learning model capable of distinguishing between pictures of bees and ants.\\nThe triton-fastapi-docker repository comprises the following services:\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/05/28/triton.html', 'title': 'Beyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog', 'description': 'Serving Models', 'language': 'en'}), Document(page_content=\"Beyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJayesh's Blog\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog Archive\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBeyond FastAPI: Using Nvidia Triton for serving ML models\\n\\nMay 28, 2023\\n      \\n      • Jayesh Mahapatra\\n\\n\\n\\nShare on: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nServing Models\\nIn today’s world, ML engineers are taking on the role of “full stack” professionals, not only developing new models but also deploying them. Python has emerged as the language of choice in the field of Machine Learning, leading engineers to gravitate towards Python-based web frameworks for model deployment. While FastAPI, a modern and high-performance web framework for building APIs in Python, has become a popular choice, it does have certain limitations when it comes to serving ML models effectively.\\n\\nThe Limitations of FastAPI\\nFastAPI, with its support for asynchronous request processing and data validation, empowers ML engineers to write REST API endpoints for ML inference. However, it wasn’t explicitly designed to serve as a dedicated inference engine. As a result, it lacks built-in support for ML frameworks and inference-specific features that can maximize throughput. Although FastAPI is ok for scenarios where high throughput isn’t critical, as ML inference scales, it becomes advisable to transition to an inference engine that is explicitly built for serving ML models to maximize performance and efficiency.\\nIntroducing Nvidia Triton: A Powerful Inference Serving Platform\\nNvidia Triton is an open-source inference serving platform that excels in optimizing and scaling machine learning model deployment. It surpasses FastAPI in several key aspects, making it an ideal solution for serving ML models efficiently:\\n\\n\\nFramework Support: Triton supports all major ML frameworks, including Tensorflow, PyTorch, TensorRT, Onnx, and more. This built-in framework support simplifies the deployment process for models developed using various ML frameworks.\\n\\n\\nDynamic Batching: Triton can batch incoming requests in an online fashion, maximizing throughput and reducing latency. This efficient handling of multiple inference requests simultaneously enhances performance.\\n\\n\\nConcurrent Model Execution: Triton enables loading multiple models and instances into GPU memory. These models can be executed in parallel, further boosting throughput and performance.\\n\\n\\nGPU Accelerated Preprocessing: Triton leverages the power of GPUs with the NVIDIA Data Loading Library (DALI) for faster data transformation and enhanced inference speed.\\n\\n\\nIntegration with Kubernetes and Prometheus: Triton seamlessly integrates with Kubernetes for efficient container orchestration, enabling scalable deployment and management of ML models. Its integration with Prometheus facilitates monitoring and observability, essential for ML operations.\\n\\n\\nHowever, triton does have some drawbacks:\\n\\nLarge Docker Image Size: Triton’s Docker images can be large (~10.6 GB), even when deploying a Triton server with specific backends. This can result in increased storage and network requirements.\\nLearning Curve: Setting up and configuring the Triton server may require some additional time and effort due to its advanced features and flexibility.\\n\\nWhile these disadvantages should be taken into account, they are outweighed by the benefits when high throughput, scalability, and efficiency are the priority.\\nTo learn more about Nvidia Triton, you can visit the official website.\\n\\nExample Repository: Deploying PyTorch Model for Bee vs. Ant Classification with Triton\\nTo demonstrate the deployment workflow of ML models using NVIDIA Triton, FastAPI, and Gradio, I have created the example repository called triton-fastapi-docker. This repository provides a practical implementation of utilizing Triton with Docker containers to deploy a PyTorch-based machine learning model capable of distinguishing between pictures of bees and ants.\\nThe triton-fastapi-docker repository comprises the following services:\", metadata={'source': 'https://jayeshmahapatra.github.io/2023/05/28/triton.html', 'title': 'Beyond FastAPI: Using Nvidia Triton for serving ML models | Jayesh’s Blog', 'description': 'Serving Models', 'language': 'en'})]}\n",
      "{'answer': ' The'}\n",
      "{'answer': ' tr'}\n",
      "{'answer': 'it'}\n",
      "{'answer': 'on'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'fast'}\n",
      "{'answer': 'api'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'd'}\n",
      "{'answer': 'ocker'}\n",
      "{'answer': ' repository'}\n",
      "{'answer': ' consists'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' three'}\n",
      "{'answer': ' main'}\n",
      "{'answer': ' services'}\n",
      "{'answer': ':'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '1'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' T'}\n",
      "{'answer': 'rit'}\n",
      "{'answer': 'on'}\n",
      "{'answer': ' In'}\n",
      "{'answer': 'ference'}\n",
      "{'answer': ' Server'}\n",
      "{'answer': ' ('}\n",
      "{'answer': 'tr'}\n",
      "{'answer': 'it'}\n",
      "{'answer': 'on'}\n",
      "{'answer': 'server'}\n",
      "{'answer': '):'}\n",
      "{'answer': ' This'}\n",
      "{'answer': ' service'}\n",
      "{'answer': ' is'}\n",
      "{'answer': ' responsible'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' running'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' N'}\n",
      "{'answer': 'vid'}\n",
      "{'answer': 'ia'}\n",
      "{'answer': ' T'}\n",
      "{'answer': 'rit'}\n",
      "{'answer': 'on'}\n",
      "{'answer': ' in'}\n",
      "{'answer': 'ference'}\n",
      "{'answer': ' server'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' serving'}\n",
      "{'answer': ' ML'}\n",
      "{'answer': ' models'}\n",
      "{'answer': ' using'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' GPU'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' faster'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' efficient'}\n",
      "{'answer': ' in'}\n",
      "{'answer': 'ference'}\n",
      "{'answer': '.'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '2'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' Fast'}\n",
      "{'answer': 'API'}\n",
      "{'answer': ' API'}\n",
      "{'answer': ' Gate'}\n",
      "{'answer': 'way'}\n",
      "{'answer': ' ('}\n",
      "{'answer': 'api'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'gate'}\n",
      "{'answer': 'way'}\n",
      "{'answer': '):'}\n",
      "{'answer': ' This'}\n",
      "{'answer': ' Fast'}\n",
      "{'answer': 'API'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'based'}\n",
      "{'answer': ' micro'}\n",
      "{'answer': 'service'}\n",
      "{'answer': ' acts'}\n",
      "{'answer': ' as'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' gate'}\n",
      "{'answer': 'way'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' expose'}\n",
      "{'answer': ' multiple'}\n",
      "{'answer': ' ML'}\n",
      "{'answer': ' model'}\n",
      "{'answer': ' end'}\n",
      "{'answer': 'points'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' clients'}\n",
      "{'answer': ','}\n",
      "{'answer': ' handling'}\n",
      "{'answer': ' rout'}\n",
      "{'answer': 'ing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' communication'}\n",
      "{'answer': ' with'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' T'}\n",
      "{'answer': 'rit'}\n",
      "{'answer': 'on'}\n",
      "{'answer': ' in'}\n",
      "{'answer': 'ference'}\n",
      "{'answer': ' server'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' It'}\n",
      "{'answer': ' also'}\n",
      "{'answer': ' handles'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' data'}\n",
      "{'answer': ' pre'}\n",
      "{'answer': 'processing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' post'}\n",
      "{'answer': 'processing'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' incoming'}\n",
      "{'answer': ' requests'}\n",
      "{'answer': '.'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '3'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' Grad'}\n",
      "{'answer': 'io'}\n",
      "{'answer': ' User'}\n",
      "{'answer': ' Inter'}\n",
      "{'answer': 'face'}\n",
      "{'answer': ' ('}\n",
      "{'answer': 'grad'}\n",
      "{'answer': 'io'}\n",
      "{'answer': '):'}\n",
      "{'answer': ' This'}\n",
      "{'answer': ' optional'}\n",
      "{'answer': ' service'}\n",
      "{'answer': ' provides'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' simple'}\n",
      "{'answer': ' web'}\n",
      "{'answer': ' UI'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' testing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' visual'}\n",
      "{'answer': 'izing'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' performance'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' ML'}\n",
      "{'answer': ' models'}\n",
      "{'answer': ' deployed'}\n",
      "{'answer': ' using'}\n",
      "{'answer': ' T'}\n",
      "{'answer': 'rit'}\n",
      "{'answer': 'on'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' Fast'}\n",
      "{'answer': 'API'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' It'}\n",
      "{'answer': ' can'}\n",
      "{'answer': ' be'}\n",
      "{'answer': ' accessed'}\n",
      "{'answer': ' through'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' browser'}\n",
      "{'answer': ' by'}\n",
      "{'answer': ' visiting'}\n",
      "{'answer': ' `'}\n",
      "{'answer': 'http'}\n",
      "{'answer': '://'}\n",
      "{'answer': 'localhost'}\n",
      "{'answer': ':'}\n",
      "{'answer': '7'}\n",
      "{'answer': '8'}\n",
      "{'answer': '5'}\n",
      "{'answer': '1'}\n",
      "{'answer': '`.'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': 'To'}\n",
      "{'answer': ' get'}\n",
      "{'answer': ' started'}\n",
      "{'answer': ','}\n",
      "{'answer': ' follow'}\n",
      "{'answer': ' these'}\n",
      "{'answer': ' steps'}\n",
      "{'answer': ':'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '1'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' Cl'}\n",
      "{'answer': 'one'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' repository'}\n",
      "{'answer': ':'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '  '}\n",
      "{'answer': ' ```'}\n",
      "{'answer': 'b'}\n",
      "{'answer': 'ash'}\n",
      "{'answer': '\\n'}\n",
      "{'answer': '  '}\n",
      "{'answer': ' git'}\n",
      "{'answer': ' clone'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m metadata \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mretrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# if \"answer\" in chunk:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     chunks.append(chunk)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#     print(chunk['answer'], end=\"\", flush=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#     metadata.append(chunk)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:4132\u001b[0m, in \u001b[0;36mRunnableBindingBase.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   4127\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4128\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4129\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4130\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 4132\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   4133\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4135\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4136\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:2424\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2420\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   2421\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2422\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2423\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2424\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:2411\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   2406\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2407\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2408\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2411\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   2414\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2415\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2416\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:1497\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1497\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:2375\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m   2367\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   2368\u001b[0m         final_pipeline,\n\u001b[1;32m   2369\u001b[0m         patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         ),\n\u001b[1;32m   2373\u001b[0m     )\n\u001b[0;32m-> 2375\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py:497\u001b[0m, in \u001b[0;36mRunnableAssign.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    494\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:1497\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1497\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py:488\u001b[0m, in \u001b[0;36mRunnableAssign._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# yield map output\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m cast(Dict[\u001b[38;5;28mstr\u001b[39m, Any], first_map_chunk_future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m--> 488\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_output\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:2793\u001b[0m, in \u001b[0;36mRunnableParallel.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   2788\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2789\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2790\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   2792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m-> 2793\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2794\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2795\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:1497\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1497\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/site-packages/langchain_core/runnables/base.py:2774\u001b[0m, in \u001b[0;36mRunnableParallel._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;66;03m# Yield chunks from each as they become available,\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;66;03m# and start the next iteration of that generator that yielded it.\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;66;03m# When all generators are exhausted, stop.\u001b[39;00m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m futures:\n\u001b[0;32m-> 2774\u001b[0m     completed_futures, _ \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2775\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m completed_futures:\n\u001b[1;32m   2776\u001b[0m         (step_name, generator) \u001b[38;5;241m=\u001b[39m futures\u001b[38;5;241m.\u001b[39mpop(future)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/concurrent/futures/_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = \"Why use arcface loss?\"\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "for chunk in retrieval_chain.stream({\"input\": query}):\n",
    "    print(chunk)\n",
    "    # if \"answer\" in chunk:\n",
    "    #     chunks.append(chunk)\n",
    "    #     print(chunk['answer'], end=\"\", flush=True)\n",
    "    # else:\n",
    "    #     metadata.append(chunk)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metadata[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
