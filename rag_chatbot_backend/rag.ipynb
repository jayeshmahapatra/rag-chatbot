{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# User defined libraries\n",
    "from sitemap_crawler import get_urls_from_sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_urls_from_sitemap('https://jayeshmahapatra.github.io/sitemap.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load websites\n",
    "website_urls = get_urls_from_sitemap('https://jayeshmahapatra.github.io/sitemap.xml')\n",
    "loader = WebBaseLoader(website_urls)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "separators = [\"\\n\\n\", \"\\n\", \"\\\\[\", \"//]\", \"\\\\(\", '\\\\)',  \" \", \"\"]\n",
    "chunk_size = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=int(chunk_size/10),\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=separators,\n",
    ")\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"mistral:instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"<s> [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. [/INST] </s> \n",
    "[INST] Question: {input} \n",
    "Context: {context} \n",
    "Answer: [/INST]\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = retrieval_chain.invoke({\"input\": \"Why use arcface loss?\"})\n",
    "# print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:...\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RoPE embeddings are positional embeddings used in transformer models, applied after the self-attention mechanism. They are computed using the rotary position embedding function `apply_rotary_emb`, with cosine and sine frequencies generated by `precompute_freqs_cis` function. These embeddings help to capture relative positional information."
     ]
    }
   ],
   "source": [
    "query = \"What are RoPE embeddings?\"\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "for chunk in retrieval_chain.stream({\"input\": query}):\n",
    "    if \"answer\" in chunk:\n",
    "        chunks.append(chunk)\n",
    "        print(chunk['answer'], end=\"\", flush=True)\n",
    "    else:\n",
    "        metadata.append(chunk)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What are RoPE embeddings?'},\n",
       " {'context': [Document(page_content='# QKV\\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\\n        xq = xq.reshape(bsz, seqlen, self.n_local_heads, self.head_dim)\\n        xk = xk.reshape(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\\n        xv = xv.reshape(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\\n\\n        # RoPE relative positional embeddings\\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)', metadata={'source': 'https://jayeshmahapatra.github.io/2023/12/03/llama2.html', 'title': 'llama2.npy : Implementing Llama2 LLM using just Python and Numpy | Jayesh‚Äôs Blog', 'description': 'Large Language Models (LLMs), such as GPT-4, Claude, and Llama2, have reshaped the landscape of Natural Language Processing (NLP), democratizing AI applications. These models often have billions of parameters and are trained on massive datasets of text, often crawled from the internet.', 'language': 'en', 'start_index': 13577}),\n",
       "   Document(page_content='Embeddings & Separability\\nBefore we start discussing losses, let‚Äôs take a refresher on what embeddings are:', metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh‚Äôs Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en', 'start_index': 806}),\n",
       "   Document(page_content='After this applying the softmax calculation and multiplying with value matrix as normal.\\n\\n\\nBelow is the python implementation of RoPE embeddings\\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\\n    freqs = 1.0 / (theta ** (np.arange(0, dim, 2)[: (dim // 2)].astype(np.float32) / dim))\\n    t = np.arange(end).astype(np.float32)\\n    freqs = np.outer(t, freqs).astype(np.float32)\\n    freqs_cos = np.cos(freqs)\\n    freqs_sin = np.sin(freqs)\\n    return freqs_cos, freqs_sin', metadata={'source': 'https://jayeshmahapatra.github.io/2023/12/03/llama2.html', 'title': 'llama2.npy : Implementing Llama2 LLM using just Python and Numpy | Jayesh‚Äôs Blog', 'description': 'Large Language Models (LLMs), such as GPT-4, Claude, and Llama2, have reshaped the landscape of Natural Language Processing (NLP), democratizing AI applications. These models often have billions of parameters and are trained on massive datasets of text, often crawled from the internet.', 'language': 'en', 'start_index': 17519}),\n",
       "   Document(page_content='Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.\\nEmbeddings can be obtained from Neural Networks trained with traditional classification losses. However, these losses do not explicitly optimize cosine distances to achieve both inter-class separability and intra-class compactness.', metadata={'source': 'https://jayeshmahapatra.github.io/2023/06/22/arcface.html', 'title': 'Enhancing Embedding Separation with ArcFace Loss | Jayesh‚Äôs Blog', 'description': 'Embeddings play a crucial role in Machine Learning by capturing and representing relationships between objects.', 'language': 'en', 'start_index': 257})]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://jayeshmahapatra.github.io/2023/05/28/triton.html\n"
     ]
    }
   ],
   "source": [
    "print(urls[1])\n",
    "unstructured_loader = UnstructuredURLLoader([urls[1]])\n",
    "web_loader = WebBaseLoader([urls[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jayesh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jayesh/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "uns_docs = unstructured_loader.load()\n",
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Beyond FastAPI: Using Nvidia Triton for serving ML models\\n\\nMay 28, 2023\\n      \\n      ‚Ä¢ Jayesh Mahapatra\\n\\nShare on:\\n\\nServing Models\\n\\nIn today‚Äôs world, ML engineers are taking on the role of ‚Äúfull stack‚Äù professionals, not only developing new models but also deploying them. Python has emerged as the language of choice in the field of Machine Learning, leading engineers to gravitate towards Python-based web frameworks for model deployment. While FastAPI, a modern and high-performance web framework for building APIs in Python, has become a popular choice, it does have certain limitations when it comes to serving ML models effectively.\\n\\nThe Limitations of FastAPI\\n\\nFastAPI, with its support for asynchronous request processing and data validation, empowers ML engineers to write REST API endpoints for ML inference. However, it wasn‚Äôt explicitly designed to serve as a dedicated inference engine. As a result, it lacks built-in support for ML frameworks and inference-specific features that can maximize throughput. Although FastAPI is ok for scenarios where high throughput isn‚Äôt critical, as ML inference scales, it becomes advisable to transition to an inference engine that is explicitly built for serving ML models to maximize performance and efficiency.\\n\\nIntroducing Nvidia Triton: A Powerful Inference Serving Platform\\n\\nNvidia Triton is an open-source inference serving platform that excels in optimizing and scaling machine learning model deployment. It surpasses FastAPI in several key aspects, making it an ideal solution for serving ML models efficiently:\\n\\nFramework Support: Triton supports all major ML frameworks, including Tensorflow, PyTorch, TensorRT, Onnx, and more. This built-in framework support simplifies the deployment process for models developed using various ML frameworks.\\n\\nDynamic Batching: Triton can batch incoming requests in an online fashion, maximizing throughput and reducing latency. This efficient handling of multiple inference requests simultaneously enhances performance.\\n\\nConcurrent Model Execution: Triton enables loading multiple models and instances into GPU memory. These models can be executed in parallel, further boosting throughput and performance.\\n\\nGPU Accelerated Preprocessing: Triton leverages the power of GPUs with the NVIDIA Data Loading Library (DALI) for faster data transformation and enhanced inference speed.\\n\\nIntegration with Kubernetes and Prometheus: Triton seamlessly integrates with Kubernetes for efficient container orchestration, enabling scalable deployment and management of ML models. Its integration with Prometheus facilitates monitoring and observability, essential for ML operations.\\n\\nHowever, triton does have some drawbacks:\\n\\nLarge Docker Image Size: Triton‚Äôs Docker images can be large (~10.6 GB), even when deploying a Triton server with specific backends. This can result in increased storage and network requirements.\\n\\nLearning Curve: Setting up and configuring the Triton server may require some additional time and effort due to its advanced features and flexibility.\\n\\nWhile these disadvantages should be taken into account, they are outweighed by the benefits when high throughput, scalability, and efficiency are the priority.\\n\\nTo learn more about Nvidia Triton, you can visit the official website.\\n\\nExample Repository: Deploying PyTorch Model for Bee vs. Ant Classification with Triton\\n\\nTo demonstrate the deployment workflow of ML models using NVIDIA Triton, FastAPI, and Gradio, I have created the example repository called triton-fastapi-docker. This repository provides a practical implementation of utilizing Triton with Docker containers to deploy a PyTorch-based machine learning model capable of distinguishing between pictures of bees and ants.\\n\\nThe triton-fastapi-docker repository comprises the following services:\\n\\ntriton: This service runs the NVIDIA Triton inference engine, serving the PyTorch ML model that distinguishes between pictures of bees and ants.\\n\\nfastapi: The Python backend communicates with triton service via gRPC, handling client requests effectively.\\n\\ngradio: This service provides a user-friendly interface using Gradio, allowing users to upload images for inference and interact with the inference pipeline seamlessly.\\n\\nBy following the instructions provided in the repository‚Äôs README, users can easily set up and deploy their own PyTorch-based ML model. Furthermore, the repository includes a Jupyter notebook located in the notebooks directory, offering a step-by-step guide on creating a TorchScript model specifically designed to distinguish between pictures of bees and ants.\\n\\nI hope this blog helped you consider triton for your future model deployments, and the repo serves as a valuable example for the same !\\n\\n<Previous PostHello World üëã\\n\\n>Next PostEnhancing Embedding Separation with ArcFace Loss', metadata={'source': 'https://jayeshmahapatra.github.io/2023/05/28/triton.html'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare by printing documents\n",
    "print(uns_docs)\n",
    "print(web_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings as ChromaSettings\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('keys.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev.config']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dev.config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma client and vectorstore\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    host=config.get('Chroma', 'host'),\n",
    "    port=config.get('Chroma', 'port'),\n",
    "    settings = ChromaSettings(\n",
    "    chroma_client_auth_provider=\"chromadb.auth.token.TokenAuthClientProvider\",\n",
    "    chroma_client_auth_credentials=os.environ.get(\"CHROMA_API_KEY\", \"not_provided\")\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = chroma_client.get_or_create_collection(config.get('Chroma', 'collection_name'))\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma schema if it does not exist\n",
    "#collection = chroma_client.get_or_create_collection()\n",
    "\n",
    "# Delete Chroma collection if it exists\n",
    "chroma_client.delete_collection(name = config.get('Chroma', 'collection_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data = collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'data', 'uris'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
